

# 教程：基于机器学习的射频无源器件建模实战
## —— 以TSMC 65nm差分变压器为例 (第二部分：模型与训练)

**目标受众**：射频/模拟IC工程师、微波技术研究人员
**前置条件**：已完成数据处理（Data Generation & Preparation），手头拥有处理好的 `.h5` 数据集。

---

### 第一章：基线模型选择 (Model Selection)

在射频领域，我们习惯于在不同的仿真器（如 Momentum vs FEM）或不同的模型精度（集总模型 vs 分布式模型）之间做选择。在机器学习中，我们同样需要选择“架构”。

#### 1. 经典机器学习代表：XGBoost (Gradient Boosting Trees)

*   **简介**：XGBoost 是一种基于决策树的集成算法。你可以把它想象成**几千个“查找表（Look-up Tables）”的加权组合**。它通过不断添加新的树来修正前一棵树预测错误的残差。
*   **在本项目中的角色 (`train_xgboost.py`)**：
    *   **优点**：对表格型数据（几何参数 -> S参数）效果极佳；训练速度极快；不需要对数据做复杂的归一化；具备“特征重要性”分析功能（告诉你哪个几何参数对性能影响最大）。
    *   **缺点**：**不可微**。这意味着我们很难将复杂的物理公式（如 Q值公式）直接嵌入到它的损失函数中进行反向传播优化。
*   **其它经典模型**：
    *   **SVM (支持向量机)**：早期常用，适合小样本，但处理高维多输出（8个S参数）较慢。
    *   **Gaussian Process (高频过程)**：常用于 Kriging 模型，提供不确定性估计，但在数万样本量下计算量呈立方级增长。

#### 2. 经典神经网络模型：MLP (Multi-Layer Perceptron)

*   **简介**：全连接神经网络。这是最基础的深度学习模型，通过层层神经元进行矩阵乘法和非线性激活。
*   **在本项目中的角色 (`model.py` - `SimpleMLP`)**：
    *   **核心优势**：**完全可微 (Differentiable)**。这是本教程的核心。因为可微，我们可以定义一个包含 $Q$ 因子的 Loss Function，强迫模型不仅拟合 S 参数曲线，还要拟合导数特性。
    *   **代码对应**：参见 `model.py` 中的 `nn.Linear` 和 `nn.ReLU` 堆叠。
*   **其它深度学习模型**：
    *   **CNN (卷积神经网络)**：擅长处理图像。
        *   *适用场景*：如果我们直接输入版图的 GDSII 图像（Pixel-based），而不是提取好的几何参数（L, W, S）。
    *   **Transformer**：擅长处理序列（如文本、语音）。
        *   *适用场景*：如果我们把频率响应看作一个“时间序列”，或者需要处理不等长的输入特征时。但在点对点（Geo+Freq -> S）预测中，MLP 往往性价比更高。

#### 3. 科研/工程中的模型选择策略

**“奥卡姆剃刀原则”：如无必要，勿增实体。**

1.  **先跑通 Baseline (XGBoost)**：用于验证数据质量。如果 XGBoost 效果很差，说明数据有问题，换 Transformer 也没用。
2.  **上 MLP (Neural Network)**：为了引入物理约束（Physics-Informed Loss）。
3.  **上复杂模型 (Transformer/GNN)**：只有当数据量极大（百万级）或输入结构非结构化（如图结构）时才考虑。

**常见的 Paper 比较方式**：
| 模型架构 | 训练时间 | 推理速度 (1k samples) | S-Param MSE | Q-Factor Error | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **XGBoost** | < 1 min | 10 ms | 1e-4 | 15% | 无法优化Q值权重 |
| **SimpleMLP**| 10 min | 5 ms | 2e-4 | **3%** | 引入KPI Loss后Q值精度提升 |

---

### 第二章：训练中的“疑难杂症” (Training Dynamics)

模型训练有它的典型问题。

#### 1. 过拟合 (Overfitting) vs. 欠拟合 (Underfitting)

*   **RF 视角类比**：
    *   **欠拟合**：就像用一阶多项式去拟合一个LC谐振曲线，模型太简单，根本抓不住特征。
        *   *表现*：训练集 Loss 和验证集 Loss 都很高，降不下去。
    *   **过拟合**：就像用100阶多项式拟合曲线，把仿真产生的数值噪声（Mesh Noise）都拟合进去了，导致曲线在该平滑的地方出现震荡。
        *   *表现*：训练集 Loss 极低，但验证集 Loss 很高（“泛化”能力差）。
*   **代码中的解决方案**：
    *   **Dropout** (`model.py`): 训练时随机“关掉”一部分神经元，防止模型死记硬背。
    *   **Early Stopping** (`train_nn.py`): 发现验证集 Loss 不再下降时，提前终止训练（类似电路优化中的停止准则）。

#### 2. 梯度消失与爆炸 (Gradient Vanishing/Exploding)

这是神经网络特有的问题。在反向传播时，误差需要一层层传回去。

*   **RF 视角类比**：类似于多级级联放大器。如果每一级增益都小于1，信号传到最后就没了（梯度消失）；如果每一级增益都很大，信号就饱和/削波了（梯度爆炸）。
*   **本项目中的特殊挑战**：
    *   $Q$ 值的计算包含除法：$Q = \text{Im}(Z)/\text{Re}(Z)$。
    *   在自谐振频率（SRF）附近，$\text{Re}(Z)$ 可能趋近于 0，导致 Q 值趋向无穷大。计算其梯度时会产生极大的数值，瞬间摧毁模型权重。
*   **代码解决方案 (`loss_functions.py`)**：
    *   **Gradient Clamping**：`torch.nn.utils.clip_grad_norm_`，限制梯度的最大模长（类似限幅器）。
    *   **Log-MSE**：对于 Q 值这种跨度极大的指标，在 Log 域计算误差（参见代码中的 `kpis_use_log_mse`）。

---

### 第三章：调参优化 (Hyperparameter Tuning)

模型结构确定后，性能的提升主要靠调参。这与在 ADS/Cadence 中做电路 Parameter Sweep 或 Optimization 非常相似。

#### 1. 常见的调参方法

*   **Grid Search (网格搜索)**：穷举法。例如 `Layer=[3,4,5]`, `Unit=[64,128]`。效率低。
*   **Random Search (随机搜索)**：在范围内随机采样。效率通常优于网格搜索。
*   **Bayesian Optimization (贝叶斯优化)**：**智能搜索**。根据已有的尝试结果，猜测哪里可能出现更好的参数。**Optuna** 就是基于此。

#### 2. 实战 Optuna (`tune_nn.py`)

Optuna 是 Python 中最流行的自动调参框架。在教程中，我们将解析 `tune_nn.py` 的核心逻辑：

*   **定义 Objective Function (目标函数)**：
    这相当于我们在电路优化器中设置的 `Goal`。
    ```python
    def objective(trial):
        # 1. 动态建议参数（类似定义变量范围）
        lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True) 
        n_layers = trial.suggest_int("n_layers", 3, 6)
        
        # 2. 构建模型与训练
        model = SimpleMLP(..., hidden_dims=[...])
        val_loss = train_model(model, ...)
        
        # 3. 返回评价指标（类似返回 Cost Function）
        return val_loss
    ```
*   **Study (研究对象)**：
    管理整个优化过程的容器。
*   **Pruning (剪枝)**：
    如果发现某组参数在训练前几轮效果就很差，Optuna 会直接终止它，节省时间（类似于仿真器发现某组参数导致电路不收敛，直接跳过）。

#### 3. 可视化工具：WandB (Weights & Biases)

虽然代码中主要使用了 TensorBoard/Json 记录，但也推荐 WandB。

*   **类比**：WandB 就是机器学习领域的 **"Data Display Window" (ADS)** 或 **"Waveform Viewer" (Virtuoso)**。
*   **功能**：
    *   实时画出 Loss 曲线。
    *   记录每一次实验的配置（Config）。
    *   **Parallel Coordinates Plot**：这种图表能直观地告诉射频工程师：“看，当 Learning Rate > 0.001 且 Batch Size < 64 时，模型往往不收敛”。

### 教程总结：RF工程师的“AI工具箱”

| RF 概念 | ML 对应概念 |
| :--- | :--- |
| 电路原理图 (Schematic) | 模型架构 (Model Architecture) |
| 器件参数 (W, L, R) | 权重与偏置 (Weights & Biases) |
| 仿真器设置 (Simulation Setup) | 超参数 (Hyperparameters) |
| 优化目标 (Optimization Goal) | 损失函数 (Loss Function) |
| 测量与拟合 (Measurement & Fitting) | 训练 (Training) |
| 泛化验证 (PVT Corner Sim) | 验证集与测试集 (Validation/Test Set) |

